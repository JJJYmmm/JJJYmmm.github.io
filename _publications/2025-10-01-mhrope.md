---
title: "Revisiting Multimodal Positional Encoding in Vision-Language Models"
collection: publications
category: preprints
permalink: /publication/2025-mhrope
excerpt: 'We propose MHRoPE and MRoPE-I, simple and plug-and-play positional encoding variants that consistently outperform existing approaches in vision-language models.'
date: 2025-10-01
venue: 'arXiv preprint'
paperurl: 'https://arxiv.org/abs/2510.23095'
citation: 'Jie Huang, Xuejing Liu, Shijie Song, Ruibing Hou, Hong Chang, Jinlin Lin, Shuai Bai. (2025). &quot;Revisiting Multimodal Positional Encoding in Vision-Language Models.&quot; <i>arXiv preprint arXiv:2510.23095</i>.'
---

We conduct a comprehensive analysis of multimodal Rotary Positional Embedding (RoPE) in vision-language models and distill three essential guidelines: positional coherence, full frequency utilization, and preservation of textual priors. Based on these insights, we propose Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), which consistently outperform existing approaches across diverse benchmarks.

[Paper](https://arxiv.org/abs/2510.23095)
