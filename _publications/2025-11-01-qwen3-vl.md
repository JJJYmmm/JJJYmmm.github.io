---
title: "Qwen3-VL Technical Report"
collection: publications
category: preprints
permalink: /publication/2025-qwen3-vl
excerpt: 'Qwen3-VL is the most capable vision-language model in the Qwen series, supporting interleaved contexts of up to 256K tokens for text, images, and video.'
date: 2025-11-01
venue: 'arXiv preprint'
paperurl: 'https://arxiv.org/abs/2511.21631'
citation: 'Shuai Bai, ..., Jie Huang, ..., et al. (2025). &quot;Qwen3-VL Technical Report.&quot; <i>arXiv preprint arXiv:2511.21631</i>.'
---

Qwen3-VL is the most capable vision-language model in the Qwen series, featuring dense variants (2B/4B/8B/32B) and mixture-of-experts variants (30B-A3B/235B-A22B). The model supports interleaved contexts of up to 256K tokens for text, images, and video, with enhanced interleaved-MRoPE for spatial-temporal modeling and DeepStack integration for multi-level ViT features.

[Paper](https://arxiv.org/abs/2511.21631) | [GitHub](https://github.com/QwenLM/Qwen3-VL)
