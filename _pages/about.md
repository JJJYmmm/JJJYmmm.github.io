---
permalink: /
title: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## About Me

I am a graduate student at the [Institute of Computing Technology (ICT)](http://www.ict.ac.cn/), Chinese Academy of Sciences, advised by [Prof. Shiguang Shan](https://scholar.google.com/citations?user=Vkzd7MIAAAAJ), [Prof. Hong Chang](https://scholar.google.com/citations?user=jE9xFT0AAAAJ) and [Dr. Ruibing Hou](https://scholar.google.com/citations?user=gJiMjfkAAAAJ). My research focuses on **Computer Vision** and **Vision-Language Models**, including human-centric perception, multimodal positional encoding, and large vision-language models.

## Research Interests

* **Vision-Language Models:** Multimodal positional encoding, large-scale VLM pre-training and architecture design **[Qwen3-VL, MHRoPE]**
* **Human-Centric Perception:** Unified referring perception framework for human-centric scenarios **[RefHCM]**
* **Adversarial Robustness:** Physical adversarial attacks in autonomous driving

## Experience

#### Education

* **Institute of Computing Technology, Chinese Academy of Sciences**: Graduate student (present)
<!-- * **[Your Undergrad University]**: B.S. (20xx - 20xx) -->

#### Internship

* **Qwen Team, Alibaba Cloud (通义千问)**: Contributed to the development of Qwen3-VL, participating in multimodal positional encoding research and model training.

## News

* **[Nov. 2025]** Our **Qwen3-VL** technical report has been released. [[Paper]](https://arxiv.org/abs/2511.21631) [[GitHub]](https://github.com/QwenLM/Qwen3-VL)
* **[Oct. 2025]** Our paper **"Revisiting Multimodal Positional Encoding in Vision-Language Models"** has been released. [[Paper]](https://arxiv.org/abs/2510.23095)
* **[Dec. 2024]** Our paper **RefHCM** has been released and accepted by **IEEE TMM**. [[Paper]](https://arxiv.org/abs/2412.14643) [[Code]](https://github.com/JJJYmmm/RefHCM)

## Publications

<small>(\* denotes equal contribution)</small>

### Vision-Language Models

<ol>
<li>
<strong>Qwen3-VL Technical Report</strong><br>
Shuai Bai, ..., <strong>Jie Huang</strong>, ..., et al.<br>
<em>arXiv preprint, 2025.</em><br>
<a href="https://arxiv.org/abs/2511.21631">[Paper]</a> <a href="https://github.com/QwenLM/Qwen3-VL">[GitHub]</a>
</li>

<li>
<strong>Revisiting Multimodal Positional Encoding in Vision-Language Models</strong><br>
<strong>Jie Huang</strong>, Xuejing Liu, Shijie Song, Ruibing Hou, Hong Chang, Jinlin Lin, Shuai Bai<br>
<em>arXiv preprint, 2025.</em><br>
<a href="https://arxiv.org/abs/2510.23095">[Paper]</a>
</li>
</ol>

### Human-Centric Perception

<ol>
<li>
<strong>RefHCM: A Unified Model for Referring Perceptions in Human-Centric Scenarios</strong><br>
<strong>Jie Huang</strong>, Ruibing Hou, Jiahe Zhao, Hong Chang, Shiguang Shan<br>
<em>IEEE Transactions on Multimedia <strong>(TMM)</strong>, 2025.</em><br>
<a href="https://arxiv.org/abs/2412.14643">[Paper]</a> <a href="https://github.com/JJJYmmm/RefHCM">[Code]</a>
</li>
</ol>

### Adversarial Robustness

<ol>
<li>
<strong>Stealthy and Effective Physical Adversarial Attacks in Autonomous Driving</strong><br>
Mingfu Zhou, Wei Zhou, <strong>Jie Huang</strong>, Jianyuan Yang, Mingxing Du, Qiang Li<br>
<em>IEEE Transactions on Information Forensics and Security <strong>(TIFS)</strong>, 19, 6795-6809, 2024.</em><br>
<a href="https://ieeexplore.ieee.org/document/10583962">[Paper]</a>
</li>
</ol>
